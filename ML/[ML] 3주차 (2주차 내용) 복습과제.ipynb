{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# 1*28*28 -> 784 = 28*28\n",
    "train_data = MNIST(root=path,train=True,transform=transform,download=True)\n",
    "test_data = MNIST(root=path,train=False,transform=transform,download=True)\n",
    "\n",
    "# choose train data with label 0 or 1\n",
    "idx = (train_data.targets==0) | (train_data.targets==1)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "\n",
    "# choose test data with label 0 or 1\n",
    "idx = (test_data.targets==0) | (test_data.targets==1)\n",
    "test_data.targets = test_data.targets[idx]\n",
    "test_data.data = test_data.data[idx]\n",
    "\n",
    "batch_size = 85\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data,batch_size=len(test_data),shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11002ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HERE ####\n",
    "# we are assuming one layer logistic regression\n",
    "w = np.random.randn(???,1)\n",
    "b = np.random.randn(???,1)\n",
    "eta = 1e-4 # learning rate\n",
    "delta = 1e-10 # prevent log 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "def sigmoid(val):\n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    ##############\n",
    "    return result\n",
    "\n",
    "#### HERE ####\n",
    "# define derivative of sigmoid function w.r.t. its value\n",
    "def grad_sigmoid(val):\n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    ##############\n",
    "    return result\n",
    "\n",
    "# given data instances in batch form,\n",
    "# compute loss and gradients of w and b\n",
    "# also, count the number of correct prediction\n",
    "def compute_loss_and_grad(data_instance):\n",
    "    x, y = data_instance\n",
    "    # fill out here and return the variables correctly anyway you want to\n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    ##############\n",
    "    return loss, (grad_w, grad_b), hit # 여기서 hit은 예측이 맞은 개수\n",
    "\n",
    "# update NN parameters w and b with SGD\n",
    "def update_parameters(params,grads):\n",
    "    w, b = params\n",
    "    grad_w, grad_b = grads\n",
    "    # fill out here and return the variables correctly anyway you want to    \n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    ##############\n",
    "    return w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591e77c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epoch = 500\n",
    "\n",
    "for i in range(num_epoch):\n",
    "\n",
    "    # train the logistic regression model\n",
    "    total_loss_train = 0\n",
    "    count = 0\n",
    "    for _, (x, y) in enumerate(train_loader):\n",
    "        # 85*1*28*28 -> -1=85,784\n",
    "        x, y = x.numpy().reshape(-1,784), y.numpy().reshape(-1,1)\n",
    "        params = (w, b)\n",
    "        # compute loss and gradients, and then update the parameters\n",
    "        # also, compute sum of the loss and the number of correct prediction in the batch\n",
    "        ##############\n",
    "        #### HERE ####\n",
    "        ##############\n",
    "    \n",
    "    #### HERE ####\n",
    "    # compute average loss and accuracy for the train dataset\n",
    "    loss_train = \n",
    "    acc_train = \n",
    "    \n",
    "    # test, or evaluate, the trained logistic regression model\n",
    "    dataiter = iter(test_loader)\n",
    "    te_images, te_labels = dataiter.next()\n",
    "    \n",
    "    #### HERE ####\n",
    "    # how can we fit te_images, te_labels to our model?\n",
    "\n",
    "    # compute loss, but you don't need to compute gradients and update parameters\n",
    "    # also, compute sum of the loss and the number of correct prediction\n",
    "    ##############\n",
    "    #### HERE ####\n",
    "    ##############\n",
    "\n",
    "    #### HERE ####\n",
    "    # compute average loss and accuracy for the test dataset\n",
    "    loss_test = \n",
    "    acc_test = \n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Epoch %d Train: %.3f / %.2f %%\"%(i,loss_train,acc_train*100))\n",
    "        print(\"Epoch %d Test: %.3f / %.2f %%\"%(i,loss_test,acc_test*100))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3272e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
